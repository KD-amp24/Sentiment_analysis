{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c46584e-123c-4cbc-bf78-e77699ebfd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Part 1: Dataset Loaded ===\n",
      "Shape: (40, 2)\n",
      "\n",
      "First 5 rows:\n",
      "      label                                               text\n",
      "0  phishing  Urgent: Your account has been compromised. Ver...\n",
      "1  phishing  We detected unusual activity. Click the link b...\n",
      "2  phishing  Your payment could not be processed. Update yo...\n",
      "3  phishing  Final notice: Failure to respond will result i...\n",
      "4  phishing  You have received a secure message. Open the a...\n",
      "\n",
      "Label values found: ['phishing', 'safe']\n",
      "\n",
      "Counts by label:\n",
      "label\n",
      "phishing    20\n",
      "safe        20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Phishing count ('phishing'): 20\n",
      "Safe count ('safe'): 20\n",
      "\n",
      "=== Examples ===\n",
      "\n",
      "--- 3 example emails for label 'phishing' ---\n",
      "\n",
      "Example 1:\n",
      "Urgent: Your account has been compromised. Verify your information immediately to restore access.\n",
      "\n",
      "Example 2:\n",
      "We detected unusual activity. Click the link below to confirm your identity.\n",
      "\n",
      "Example 3:\n",
      "Your payment could not be processed. Update your billing details now.\n",
      "\n",
      "--- 3 example emails for label 'safe' ---\n",
      "\n",
      "Example 1:\n",
      "Hi team, please find the meeting agenda attached for tomorrow’s discussion.\n",
      "\n",
      "Example 2:\n",
      "Your library book is due for return next week.\n",
      "\n",
      "Example 3:\n",
      "Reminder: Staff training session scheduled for Friday.\n",
      "\n",
      "=== Part 2: Preprocessing with NLTK ===\n",
      "\n",
      "Showing original email + cleaned token list for first 3 samples:\n",
      "\n",
      "Sample 1 (label=phishing)\n",
      "Original email:\n",
      "Urgent: Your account has been compromised. Verify your information immediately to restore access.\n",
      "Cleaned token list:\n",
      "['urgent', 'account', 'compromised', 'verify', 'information', 'immediately', 'restore', 'access']\n",
      "------------------------------------------------------------\n",
      "Sample 2 (label=phishing)\n",
      "Original email:\n",
      "We detected unusual activity. Click the link below to confirm your identity.\n",
      "Cleaned token list:\n",
      "['detected', 'unusual', 'activity', 'click', 'link', 'confirm', 'identity']\n",
      "------------------------------------------------------------\n",
      "Sample 3 (label=phishing)\n",
      "Original email:\n",
      "Your payment could not be processed. Update your billing details now.\n",
      "Cleaned token list:\n",
      "['payment', 'could', 'processed', 'update', 'billing', 'details']\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Part 3: Sentiment Analysis (VADER) ===\n",
      "\n",
      "Average sentiment_score for phishing ('phishing'): 0.1042\n",
      "Average sentiment_score for safe ('safe'): 0.1678\n",
      "\n",
      "Average sentiment_score by label:\n",
      "label\n",
      "safe        0.167805\n",
      "phishing    0.104210\n",
      "Name: sentiment_score, dtype: float64\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Amponsah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Amponsah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Amponsah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Amponsah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phishing vs Safe Email Sentiment + Language Pattern Starter Pipeline (NLTK)\n",
    "\n",
    "What this script does (Parts 1–3):\n",
    "------------------------------------------------------------\n",
    "Part 1 - Load & inspect:\n",
    "  - Load CSV with pandas\n",
    "  - Show first 5 rows\n",
    "  - Show class counts (phishing vs safe/other)\n",
    "  - Print 3 example phishing emails and 3 example safe emails\n",
    "\n",
    "Part 2 - Preprocessing with NLTK:\n",
    "  - Tokenize\n",
    "  - Lowercase\n",
    "  - Remove stopwords\n",
    "  - Remove punctuation\n",
    "  - Show original email + cleaned token list (sample)\n",
    "\n",
    "Part 3 - Sentiment analysis with VADER:\n",
    "  - Compute sentiment score (compound) for each email\n",
    "  - Add sentiment_score column\n",
    "  - Compute average sentiment by class (phishing vs safe)\n",
    "\n",
    "How to run:\n",
    "------------------------------------------------------------\n",
    "python phishing_sentiment_pipeline.py --csv_path \"your_file.csv\"\n",
    "\n",
    "Optional:\n",
    "python phishing_sentiment_pipeline.py --csv_path \"your_file.csv\" --phishing_label \"phishing\" --safe_label \"safe\"\n",
    "\"\"\"\n",
    "\n",
    "import string\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# HARDCODED DATASET PATH\n",
    "# ============================================================\n",
    "CSV_PATH = r\"D:\\Year 2 semester 2\\Sys. and Proj\\sentitment analysis\\sample_email_dataset.csv\"\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# NLTK setup helpers\n",
    "# -------------------------------\n",
    "def ensure_nltk_resources() -> None:\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/punkt\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\")\n",
    "\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/punkt_tab\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt_tab\")\n",
    "\n",
    "    try:\n",
    "        nltk.data.find(\"corpora/stopwords\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"stopwords\")\n",
    "\n",
    "    try:\n",
    "        nltk.data.find(\"sentiment/vader_lexicon\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"vader_lexicon\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Text preprocessing\n",
    "# -------------------------------\n",
    "def preprocess_email(text: str, stop_words: set) -> List[str]:\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if text is None else str(text)\n",
    "\n",
    "    # 1) Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    for tok in tokens:\n",
    "        # 2) Lowercasing\n",
    "        tok = tok.lower()\n",
    "\n",
    "        # Remove punctuation\n",
    "        tok = tok.strip(string.punctuation)\n",
    "\n",
    "        if not tok:\n",
    "            continue\n",
    "\n",
    "        # 3) Remove stopwords\n",
    "        if tok in stop_words:\n",
    "            continue\n",
    "\n",
    "        cleaned_tokens.append(tok)\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Label normalization\n",
    "# -------------------------------\n",
    "def normalize_label(label_val: str) -> str:\n",
    "    if label_val is None:\n",
    "        return \"\"\n",
    "    return str(label_val).strip().lower()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Main pipeline\n",
    "# -------------------------------\n",
    "def main():\n",
    "\n",
    "    ensure_nltk_resources()\n",
    "\n",
    "    # ---------------------------------\n",
    "    # Part 1: Load & inspect data\n",
    "    # ---------------------------------\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "    # Column names (modify here if needed)\n",
    "    LABEL_COL = \"label\"\n",
    "    TEXT_COL = \"text\"\n",
    "\n",
    "    if LABEL_COL not in df.columns or TEXT_COL not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"CSV must contain columns '{LABEL_COL}' and '{TEXT_COL}'. \"\n",
    "            f\"Found columns: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    df[LABEL_COL] = df[LABEL_COL].apply(normalize_label)\n",
    "\n",
    "    print(\"\\n=== Part 1: Dataset Loaded ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df[[LABEL_COL, TEXT_COL]].head(5))\n",
    "\n",
    "    unique_labels = sorted(df[LABEL_COL].dropna().unique().tolist())\n",
    "\n",
    "    print(\"\\nLabel values found:\", unique_labels)\n",
    "\n",
    "    print(\"\\nCounts by label:\")\n",
    "    print(df[LABEL_COL].value_counts(dropna=False))\n",
    "\n",
    "    # Identify phishing and safe automatically\n",
    "    phishing_label = \"phishing\"\n",
    "    safe_label = None\n",
    "\n",
    "    for lab in unique_labels:\n",
    "        if lab != phishing_label:\n",
    "            safe_label = lab\n",
    "            break\n",
    "\n",
    "    if safe_label:\n",
    "        phishing_count = int((df[LABEL_COL] == phishing_label).sum())\n",
    "        safe_count = int((df[LABEL_COL] == safe_label).sum())\n",
    "        print(f\"\\nPhishing count ('{phishing_label}'): {phishing_count}\")\n",
    "        print(f\"Safe count ('{safe_label}'): {safe_count}\")\n",
    "\n",
    "    # Print example emails\n",
    "    def print_examples(label_name: str, n: int = 3):\n",
    "        subset = df[df[LABEL_COL] == label_name]\n",
    "        if subset.empty:\n",
    "            return\n",
    "        print(f\"\\n--- {n} example emails for label '{label_name}' ---\")\n",
    "        for i, txt in enumerate(subset[TEXT_COL].head(n).tolist(), start=1):\n",
    "            print(f\"\\nExample {i}:\\n{txt}\")\n",
    "\n",
    "    print(\"\\n=== Examples ===\")\n",
    "    print_examples(phishing_label, 3)\n",
    "    if safe_label:\n",
    "        print_examples(safe_label, 3)\n",
    "\n",
    "    # ---------------------------------\n",
    "    # Part 2: Preprocessing\n",
    "    # ---------------------------------\n",
    "    print(\"\\n=== Part 2: Preprocessing with NLTK ===\")\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    df[\"clean_tokens\"] = df[TEXT_COL].apply(lambda t: preprocess_email(t, stop_words))\n",
    "\n",
    "    print(\"\\nShowing original email + cleaned token list for first 3 samples:\\n\")\n",
    "\n",
    "    for idx in range(min(3, len(df))):\n",
    "        original = df.iloc[idx][TEXT_COL]\n",
    "        tokens = df.iloc[idx][\"clean_tokens\"]\n",
    "        label_val = df.iloc[idx][LABEL_COL]\n",
    "        print(f\"Sample {idx+1} (label={label_val})\")\n",
    "        print(\"Original email:\")\n",
    "        print(original)\n",
    "        print(\"Cleaned token list:\")\n",
    "        print(tokens)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    # ---------------------------------\n",
    "    # Part 3: Sentiment Analysis\n",
    "    # ---------------------------------\n",
    "    print(\"\\n=== Part 3: Sentiment Analysis (VADER) ===\")\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def vader_compound_score(text: str) -> float:\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\" if text is None else str(text)\n",
    "        return float(sia.polarity_scores(text)[\"compound\"])\n",
    "\n",
    "    df[\"sentiment_score\"] = df[TEXT_COL].apply(vader_compound_score)\n",
    "\n",
    "    phishing_avg = df.loc[df[LABEL_COL] == phishing_label, \"sentiment_score\"].mean()\n",
    "    print(f\"\\nAverage sentiment_score for phishing ('{phishing_label}'): {phishing_avg:.4f}\")\n",
    "\n",
    "    if safe_label:\n",
    "        safe_avg = df.loc[df[LABEL_COL] == safe_label, \"sentiment_score\"].mean()\n",
    "        print(f\"Average sentiment_score for safe ('{safe_label}'): {safe_avg:.4f}\")\n",
    "\n",
    "    print(\"\\nAverage sentiment_score by label:\")\n",
    "    print(df.groupby(LABEL_COL)[\"sentiment_score\"].mean().sort_values(ascending=False))\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9d8991-12ff-4df7-b65d-6b278c969832",
   "metadata": {},
   "source": [
    "What This Pipeline Does\n",
    "\n",
    "\n",
    "This pipeline analyzes phishing and safe emails to understand how their language differs, how emotional tone differs and whether phishing emails use more urgency or pressure. It does this in three major stages:\n",
    "\n",
    "\n",
    "PART 1 - Loading and Inspecting the Data\n",
    "\n",
    "1.\tLoading the Dataset: The script uses pandas to load the CSV file. Once loaded, pandas stores everything in a DataFrame, which works like a spreadsheet inside Python.\n",
    "\n",
    "\n",
    "2.\tInspecting the Dataset: The script prints the dataset shape and the first 5 rows. This confirms that labels are correct and that emails are loaded properly.\n",
    "\n",
    "\n",
    "3.\tCounting Email Types: It then counts how many phishing vs safe emails exist.\n",
    "\n",
    "\n",
    "\n",
    "4.\tPrinting Example Emails: The script prints 3 phishing and 3 safe emails.\n",
    "\n",
    "\n",
    "\n",
    "PART 2 - Text Preprocessing (Cleaning the Emails)\n",
    "\n",
    "Before analyzing language, the emails must be cleaned. Raw text contains capital letters, punctuation, common filler words, formatting noise etc. Computers work better with structured tokens.\n",
    "\n",
    "1.\tTokenization: The email is split into individual words.\n",
    "Example:\n",
    "\n",
    "\n",
    "Urgent: The account has been compromised.\n",
    "\n",
    "Becomes:\n",
    "\n",
    "[‘Urgent’, ‘:’, ‘The’, ‘account’, ‘has’, ‘been’, ‘compromised’, ‘.’]\n",
    "\n",
    "\n",
    "3.\tLowercasing: All words are converted to lowercase.\n",
    "\n",
    "\n",
    "4.\tRemoving Punctuation: Symbols like ‘:’, ‘.’, ‘,’, ‘!’ etc. are removed because they don’t carry strong meaning for this analysis.\n",
    "\n",
    "\n",
    "5.\tRemoving Stopwords: Stopwords are very common words like ‘the’, ‘is’, ‘has’, ‘been’ etc. They appear in almost every sentence and don’t help distinguish phishing from safe emails.\n",
    "\n",
    "After cleaning:\n",
    "\n",
    "‘Urgent: The account has been compromised.’\n",
    "\n",
    "Becomes:\n",
    "\n",
    "[‘urgent’, ‘account’, ‘compromised’, ‘verify’, ‘information’, ‘immediately’, ‘restore’, ‘access’]\n",
    "\n",
    "\n",
    "PART 3 - Sentiment Analysis (Using VADER)\n",
    "\n",
    "This measures the emotional tone.\n",
    "The script uses:\n",
    "NLTK’s VADER (Valence Aware Dictionary for Sentiment Reasoning)\n",
    "\n",
    "VADER assigns four scores to each email:\n",
    "\n",
    "•\tpositive\n",
    "•\tnegative\n",
    "•\tneutral\n",
    "•\tcompound (overall score)\n",
    "\n",
    "\n",
    "The pipeline uses the compound score, which ranges from -1 to 1.\n",
    "\n",
    "The script calculates the average sentiment for phishing emails and for safe emails.\n",
    "\n",
    "This is a structured linguistic analysis system that cleans email text, extracts meaningful language features, computes emotional tone, and statistically compares phishing vs safe communication patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8beaf8-f039-4255-85e1-1f1511ba6884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
